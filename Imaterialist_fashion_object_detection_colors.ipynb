{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5c2d4e50-6a4a-4d40-b2f4-5eb2b31a2b2a",
        "_uuid": "ef527f58d578d6a83a361634acf38119b31f77c5",
        "id": "AGVTMTaAa9Q5"
      },
      "source": [
        "\n",
        "# **Notebook and API set up** "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle # installing kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyiz4h-VdcfO",
        "outputId": "b8ec9e2b-b046-41c9-c591-9e6d0d154591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing files\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "cxQ4z4vOdhqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading a json file download from Kaggel\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "xpndBJHwdmGa",
        "outputId": "dbe0e36f-0eea-415e-e0f7-f02709349d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e1cea4a7-c5be-41e0-b713-df8f894607e6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e1cea4a7-c5be-41e0-b713-df8f894607e6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"susaiignesh\",\"key\":\"4e82e7c97143bc0c1269429e476e9692\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this will create floder for us to save a json file\n",
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "u0lYisdud62u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cp command actully used to copy the file into the new folder that we're going to create\n",
        "\n",
        "!cp /content/kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "_fitstWPd66q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finaly we will call the chmod 600 to allow us to read and wirte to the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Zr2lGpLQd6-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for download the datasets \n",
        "#!kaggle competitions download -c (dataset name or path)\n",
        "\n",
        "!kaggle competitions download -c imaterialist-challenge-fashion-2018"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lAqyoTreTdk",
        "outputId": "d34490f3-18a5-4494-aacf-6a367f94d96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading imaterialist-challenge-fashion-2018.zip to /content\n",
            " 92% 27.0M/29.4M [00:00<00:00, 40.0MB/s]\n",
            "100% 29.4M/29.4M [00:00<00:00, 38.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip imaterialist-challenge-fashion-2018.zip # unziping the dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrdvqzzUeTgQ",
        "outputId": "bbc4ffc4-c328-4c7c-9c15-42c0546c9bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  imaterialist-challenge-fashion-2018.zip\n",
            "  inflating: sample_submission.csv.zip  \n",
            "  inflating: test.json.zip           \n",
            "  inflating: train.json.zip          \n",
            "  inflating: validation.json.zip     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## unziping the dataset\n",
        "!unzip test.json.zip\n",
        "!unzip train.json.zip\n",
        "!unzip validation.json.zip \n",
        "!unzip sample_submission.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVeLRcieTi_",
        "outputId": "6e34f046-c15a-4d8a-d02a-5c20e9bc7d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test.json.zip\n",
            "replace test.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: test.json               \n",
            "Archive:  train.json.zip\n",
            "  inflating: train.json              \n",
            "Archive:  validation.json.zip\n",
            "  inflating: validation.json         \n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://theknclan.com/wp-content/uploads/2017/10/635980679147435890-488367249_FashionHeader.png)\n",
        "\n",
        "# Extensive EDA of iMaterialist (Fashion) Dataset with Object Detection and Color Analysis\n",
        "\n",
        "This notebook contains the exploration of iMaterialist Challenge (Fashion) at FGVC5 [dataset](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018)\n",
        "\n",
        "About the iMaterialist (Fashion) Competition - \n",
        "\n",
        "As shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions.\n",
        "\n",
        "Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification.\n",
        "\n",
        "In this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Contents**\n",
        "\n",
        "**1. Descriptive Statistics**   \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;  1.1 Counts of Images and Labels  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     1.2 Top Labels in the dataset  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     1.3 Most Common Co-occuring Labels  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     1.4 Images with maxium Labels  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     1.5 Images with single Label  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     1.6 Freq Dist of Images in different label count buckets  \n",
        "**2. Colors Used in the Images**     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     2.1 Top Average Color of the images  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     2.2 Dominant Colors present in the images  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     2.3 Common Color Palletes    \n",
        "**3. Object Detection**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     3.1 Top Colors Detected in the images  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;     3.2 Top Objects Detected in the images  "
      ],
      "metadata": {
        "id": "ntWWH6HobB-Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2b988610-89eb-4d0f-a32e-4460252360c7",
        "_uuid": "f1f8b429f61feb7c2425f4ce78aac2d3a9fabc16",
        "id": "r47BbCZ7a9RP"
      },
      "source": [
        "## **Dataset Preparation** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7973e793-46ce-47e0-bab6-4c0652dc28fa",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "bcd29715fcb4f4d134ad0314411e054e4487ec59",
        "collapsed": true,
        "id": "AIFuKQLYa9RR"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML #for displaying HTML content inside jupyter notebook\n",
        "from IPython.display import Image #for displaying an image with various fromat\n",
        "from collections import Counter #for counting the occurance of each element\n",
        "import pandas as pd #for data manupulation and analysis\n",
        "import json #for encoding and decoding Json data\n",
        "\n",
        "#importing the necessary library for plotting purpose\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "import matplotlib.pyplot as plt \n",
        "import plotly.graph_objs as go\n",
        "from wordcloud import WordCloud\n",
        "from plotly import tools\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf #for training neural network model\n",
        "import numpy as np\n",
        "\n",
        "init_notebook_mode(connected=True) #initialize plotly's notebook\n",
        "%matplotlib inline #enables inline rendering of matplotlib plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1a76c5a6-b49e-47eb-a12a-37b7a8c3e137",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "1b8ee699cdf9d40b61a95a7fc6000e9687d48a51",
        "collapsed": true,
        "id": "8zl5L71aa9RU"
      },
      "outputs": [],
      "source": [
        "## read the dataset \n",
        "\n",
        "train_path = '../content/train.json' #path of the training dataset\n",
        "test_path = '../content/test.json' #path of testing dataset\n",
        "valid_path = '../content/validation.json' #path of validation set\n",
        "\n",
        "train_inp = open(train_path).read() #opens the file in the training path and read the objects in string format\n",
        "train_inp = json.loads(train_inp) #reading the json file into python object \n",
        "\n",
        "test_inp = open(test_path).read() #opens the file in the test path and read the objects in string format\n",
        "test_inp = json.loads(test_inp) #reading the json file into python object \n",
        "\n",
        "valid_inp = open(valid_path).read() #opens the file in the validation path and read the objects in string format\n",
        "valid_inp = json.loads(valid_inp) #reading the json file into python object "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6b2e0af3-ffdf-4b91-999f-fd1267909a0a",
        "_uuid": "c7452f7f723674521eb44bf4b99e00c9def6b6be",
        "id": "Y7S_Gdhya9RV"
      },
      "source": [
        "## 1. Descriptive Statistics\n",
        "\n",
        "## 1.1 How many Images and how many distinct labels are there in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9126a503-7931-471c-b1a9-83574aa99ee9",
        "_kg_hide-input": true,
        "_uuid": "443c50e2b95f2ca514b6a389b3bc4bcf604e7a5a",
        "collapsed": true,
        "id": "Qienru7za9RV"
      },
      "outputs": [],
      "source": [
        "# how many images \n",
        "def get_stats(data):\n",
        "    total_images = len(data['images']) #counting the number of images \n",
        "\n",
        "    all_annotations = [] #creating empty list \n",
        "    if 'annotations' in data:\n",
        "        for each in data['annotations']:\n",
        "            all_annotations.extend(each['labelId']) \n",
        "            #if the data has annotation in it the it iterates through the data and add the labelID of the input data to the empty list\n",
        "    total_labels = len(set(all_annotations)) #counting the number of labelid's\n",
        "    return total_images, total_labels, all_annotations\n",
        "\n",
        "total_images, total_labels, train_annotations = get_stats(train_inp) #applying the above created function to the training data\n",
        "print (\"Total Images in the train:\", total_images)\n",
        "print (\"Total Labels in the train:\", total_labels)\n",
        "print (\"\")\n",
        "\n",
        "total_images, total_labels, test_annotations = get_stats(test_inp) #applying the above created function to the test data\n",
        "print (\"Total Images in the test:\", total_images)\n",
        "print (\"Total Labels in the test:\", total_labels)\n",
        "print (\"\")\n",
        "\n",
        "total_images, total_labels, valid_annotations = get_stats(valid_inp) #applying the above created function to the validation data\n",
        "print (\"Total Images in the valid:\", total_images)\n",
        "print (\"Total Labels in the valid:\", total_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fd59fd68-c93f-45c9-b2aa-30cb15b1e5d8",
        "_uuid": "1f579316bda0134876400b8d0da547cfb4ae7fc4",
        "id": "9CYgejkya9RX"
      },
      "source": [
        "There are about 1 Million images provided in the train dataset and there are 228 distinct labels which are used to label these images. There are two other sources of data as well - test data and validation data but in thie notebook I have only used images from train dataset.\n",
        "\n",
        "## 1.2 Which are the top used Labels in the dataset ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e33a7250-839d-4d30-9166-65bf77a33af0",
        "_kg_hide-input": true,
        "_uuid": "b4648f100090276d7aa525fee9c4dc9da5f0e056",
        "collapsed": true,
        "scrolled": false,
        "id": "GSEQl_RVa9RY"
      },
      "outputs": [],
      "source": [
        "train_labels = Counter(train_annotations) #counting the number of occurrences of each element\n",
        "\n",
        "xvalues = list(train_labels.keys()) #Splitting the x column\n",
        "yvalues = list(train_labels.values()) #splitting the y column\n",
        "\n",
        "#the below code is used to create a horizontal bar chart trace that can be added to a Plotly figure to visualize data.\n",
        "trace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\"year count\", marker=dict(color='rgba(20, 20, 20, 1)'))\n",
        "\n",
        "#setting thr layout for the avove created horizontal bar\n",
        "layout = dict(width=800, title='Distribution of different labels in the train dataset', legend=dict(orientation=\"h\"));\n",
        "\n",
        "fig = go.Figure(data=[trace1], layout=layout);\n",
        "iplot(fig); #displaying the horizontal bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0417ec33-99c5-474d-8a57-26271437f100",
        "_kg_hide-input": true,
        "_uuid": "a6de49820862813fa90cd8f537b14135dfbd4230",
        "collapsed": true,
        "id": "XTfMCk1_a9RY"
      },
      "outputs": [],
      "source": [
        "#repeating the above code for validation data\n",
        "valid_labels = Counter(valid_annotations) #counting the number of occurances of each element\n",
        "\n",
        "xvalues = list(valid_labels.keys()) #splitting X column\n",
        "yvalues = list(valid_labels.values()) #splitting y column\n",
        "#creating horizontal bar chart trace that can be added to a Plotly figure to visualize data\n",
        "trace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\"year count\", marker=dict(color='rgba(20, 20, 20, 1)'))\n",
        "#setting the layout for the avove created horizontal bar\n",
        "layout = dict(width=800, title='Distribution of different labels in the valid dataset', legend=dict(orientation=\"h\"));\n",
        "\n",
        "fig = go.Figure(data=[trace1], layout=layout);\n",
        "iplot(fig); #displaying the horizontal bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee7ff814-a588-4651-ad4c-41d778ce11ab",
        "_kg_hide-input": true,
        "_uuid": "cdc8029332c700bc30ee19a87224f0c5ed43744a",
        "collapsed": true,
        "id": "6c7cqDfBa9RZ"
      },
      "outputs": [],
      "source": [
        "def get_images_for_labels(labellist, data): #creating a function for getting image labels\n",
        "    image_ids = [] #creating an empty function\n",
        "    for each in data['annotations']: #iterarting through data having annotations in it\n",
        "        if all(x in each['labelId'] for x in labellist): \n",
        "            #checking if all the label IDs in labellist are present in the image's label ID list\n",
        "            image_ids.append(each['imageId']) #if, so the image ID is added to the abive created empty list\n",
        "            if len(image_ids) == 2: #if the image id has two elements then the loop breaks\n",
        "                break\n",
        "    image_urls = []\n",
        "    for each in data['images']: #iterating to through the data having images\n",
        "        if each['imageId'] in image_ids: #if image ID is in the list image_ids then it is appended to image_urls list\n",
        "            image_urls.append(each['url'])\n",
        "    return image_urls #returning the appended list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b00b8167-9a2c-4c00-974a-43dc734e56bd",
        "_kg_hide-input": true,
        "_uuid": "8516dc1796f7b979aaac060fedf70cc3863656f6",
        "collapsed": true,
        "id": "u5SX8jb7a9Ra"
      },
      "outputs": [],
      "source": [
        "# most common labels \n",
        "\n",
        "temps = train_labels.most_common(10) #getting the top 10 most common labels\n",
        "#creating two function with the top 10 most common labels\n",
        "labels_tr = [\"Label-\"+str(x[0]) for x in temps]\n",
        "values = [x[1] for x in temps]\n",
        "#creating horizontal bar chart trace that can be added to a Plotly figure to visualize data\n",
        "trace1 = go.Bar(x=labels_tr, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\n",
        "#setting the layout for the avove created horizontal bar\n",
        "layout = dict(height=400, title='Top 10 Labels in the train dataset', legend=dict(orientation=\"h\"));\n",
        "\n",
        "fig = go.Figure(data=[trace1], layout=layout);\n",
        "iplot(fig);#visualizig the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7fe989cb-dcd2-406c-b5ff-25e7d4697b43",
        "_uuid": "f87512abf884fb5ad1ffbf00085ff0824b84a09c",
        "id": "i54hrUXka9Rb"
      },
      "source": [
        "Label 66 is the most used label with almost 750K images tagged with this label in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "09bce5db-64d4-41fa-90e6-0ccf4bf8236c",
        "_uuid": "c5a56917573ede6f1709883ae843e4ba0d5cab0c",
        "collapsed": true,
        "id": "2AL-E_BTa9Rb"
      },
      "outputs": [],
      "source": [
        "temps = valid_labels.most_common(10) #getting the top 10 most common labels\n",
        "#creating two function with the top 10 most common labels\n",
        "labels_vl = [\"Label-\"+str(x[0]) for x in temps]\n",
        "values = [x[1] for x in temps]\n",
        "\n",
        "#creating horizontal bar chart trace that can be added to a Plotly figure to visualize data\n",
        "trace1 = go.Bar(x=labels_vl, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\n",
        "#setting the layout for the avove created horizontal bar\n",
        "layout = dict(height=400, title='Top 10 Labels in the valid dataset', legend=dict(orientation=\"h\"));\n",
        "\n",
        "fig = go.Figure(data=[trace1], layout=layout);\n",
        "iplot(fig); #visualizig the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c93a82a-bcbf-42e5-8104-85c4579c2256",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "b54dccf0df42f58927e22e6ddb59b1adfce2a6b9",
        "id": "qRgPRtp8a9Rd"
      },
      "source": [
        "Again, in the validation dataset, Label 66 is the most used label but second most label used is label-17 not label-105 of training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "23160b0b-cc0b-4120-82aa-1e235623921f",
        "_uuid": "eb7366687f30cc4ea04cc19490a8ca0545687e16",
        "id": "vI-tioQWa9Rd"
      },
      "source": [
        "## 1.3 What are the most Common Co-Occuring Labels in the dataset\n",
        "\n",
        "Since every image can be classified into multiple labels, it will be interesting to note which lables have co-occured together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2a73cf6c-c8bc-454f-917d-bc6d8c40840b",
        "_kg_hide-input": true,
        "_uuid": "a842fbc6fa760de7d63149861a67abf778368dc1",
        "collapsed": true,
        "id": "eq-gcdfba9Re"
      },
      "outputs": [],
      "source": [
        "# Most Commonly Occuring Labels \n",
        "\n",
        "def cartesian_reduct(alist):\n",
        "    results = []\n",
        "    for x in alist:\n",
        "        for y in alist: #iterating through the list twice\n",
        "            if x == y: #if x and y are equal then skipping and contonuing to next iteration\n",
        "                continue\n",
        "            srtd = sorted([int(x),int(y)]) #two elements x and y are sorted\n",
        "            srtd = \" AND \".join([str(x) for x in srtd]) #x and y are joined function\n",
        "            results.append(srtd) #appending the joined function to the previously created empty list\n",
        "    return results \n",
        "\n",
        "co_occurance = []\n",
        "for i, each in enumerate(train_inp['annotations']): #iterating through each iteration in train_inp\n",
        "    #extracting the label IDs using the key labelId and passes them as an argument to the cartesian_reduct function\n",
        "    prods = cartesian_reduct(each['labelId'])\n",
        "    #which returns a list of all possible pairs of label IDs as strings joined by the string \" AND\n",
        "    #The list of pairs is then appended to the co_occurance list using the extend method, \n",
        "    #which adds all the elements of the given list to the end of the co_occurance list.\n",
        "    co_occurance.extend(prods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4e2115cb-42b8-493d-a778-e69f6b2957f0",
        "_kg_hide-input": true,
        "_uuid": "52135727afb5e93b63e0b096eed0bc4addb0ad55",
        "collapsed": true,
        "id": "j-mqsZwEa9Re"
      },
      "outputs": [],
      "source": [
        "coocur = Counter(co_occurance).most_common(10)#getting the top 10 most common labels\n",
        "#creating a list of the top 10 most common pairs of labels in the train_inp dataset in reverse order\n",
        "#The label ID is obtained from the first element of each tuple in the coocur list using the index [0]\n",
        "labels = list(reversed([\"Label: \"+str(x[0]) for x in coocur]))\n",
        "#reversing the values in the coocur list \n",
        "values = list(reversed([x[1] for x in coocur]))\n",
        "\n",
        "#creating horizontal bar chart trace that can be added to a Plotly figure to visualize data\n",
        "trace1 = go.Bar(x=values, y=labels, opacity=0.7, orientation=\"h\", name=\"year count\", marker=dict(color='rgba(130, 130, 230, 0.8)'))\n",
        "layout = dict(height=400, title='Most Common Co-Occuring Labels in the dataset', legend=dict(orientation=\"h\"));\n",
        "\n",
        "fig = go.Figure(data=[trace1], layout=layout);\n",
        "iplot(fig);#visualizing the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "af51d714-c1d6-4cac-a0ae-f43541fc0717",
        "_uuid": "e126017764613c9f8f71252a0635c483abf62d51",
        "collapsed": true,
        "id": "w4mh_a4ma9Rf"
      },
      "source": [
        "From the above graph, (label 66 and label 105) and (label 66 and label 171) have been used most number of times while labelling the images, with the total count of 460K and 445K respectively. Apart from the most frequently occuring label \"66\", label 105 and label 153 have been used repeatedly in the dataset.\n",
        "\n",
        "## 1.4 Which Images are tagged with Maximum Labels\n",
        "\n",
        "Some images are labelled with single label but some images can have labels as high as 20. Lets get the images having the largest numbers of labels in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ca70481-f224-4489-95e5-edb36f3722ad",
        "_kg_hide-input": true,
        "_uuid": "1065f594debe3214532502ab9eb57bf938f7f378",
        "collapsed": true,
        "id": "sFH8uy2sa9Rf"
      },
      "outputs": [],
      "source": [
        "def get_image_url(imgid, data):\n",
        "    for each in data['images']: #iterating through the images in data\n",
        "        if each['imageId'] == imgid:\n",
        "            #if the imageid of data is equal to the vaulues of list in ImageID then the url of data is returned\n",
        "            return each['url']\n",
        "        \n",
        "#sorting the train_inp dataset based on the length of the label IDs in each annotation.\n",
        "srtedlist = sorted(train_inp['annotations'], key=lambda d: len(d['labelId']), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fa5a40e-1445-4e53-a7f2-b0f3fc577423",
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "7dd171bd3b27bf5a64ac05cdae1f7410f54718ac",
        "collapsed": true,
        "id": "PzBuspVPa9Rg"
      },
      "outputs": [],
      "source": [
        "for img in srtedlist[:5]: #iterating to the first 5 elements in stredlist\n",
        "    #applying the above created function which iterates through images in data \n",
        "    #and returns the url of the image if the imageid in dataset is equal to the list imageid\n",
        "    iurl = get_image_url(img['imageId'], train_inp) \n",
        "    #joining the imageid sepetrated with comma and storing it in the variable called labelpair\n",
        "    labelpair = \", \".join(img['labelId'])\n",
        "    #The imghtml variable is created using a formatted string that includes the labelpair and the length of the labelId list in the annotation, as well as the iurl. \n",
        "    imghtml = \"\"\"Labels: \"\"\"+ str(labelpair) +\"\"\" &nbsp;&nbsp; <b>Total Labels: \"\"\"+ str(len(img['labelId'])) + \"\"\"</b><br>\"\"\" + \"<img src=\"+iurl+\" width=200px; style='float:left'>\"\n",
        "    #displaying the HTML code for the imghtml variable\n",
        "    display(HTML(imghtml))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "34185100-fc53-400c-b576-5f6f1d685377",
        "_uuid": "5cd086ea03137d46973c8f85db823bdd1cdfa4e2",
        "id": "yp-tyOQqa9Rh"
      },
      "source": [
        "## 1.5 Which Images have perfect label ie. a Single Label\n",
        "\n",
        "Lets get some of the images which has only one label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e63b3fe2-9dbc-4da4-967e-98ae058c6985",
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "fe2cc9aadffba1e94557bbece36200de69531d5f",
        "collapsed": true,
        "id": "NQg1LpVMa9Rh"
      },
      "outputs": [],
      "source": [
        "# How many images are labelled with only 1 label \n",
        "for img in srtedlist[-5:]: #iterating to the first 5 elements in stredlist\n",
        "    #applying the above created function which iterates through images in data \n",
        "    #and returns the url of the image if the imageid in dataset is equal to the list imageid\n",
        "    iurl = get_image_url(img['imageId'], train_inp)\n",
        "    #joining the imageid sepetrated with comma and storing it in the variable called labelpair\n",
        "    labelpair = \", \".join(img['labelId'])\n",
        "    #The imghtml variable is created using a formatted string that includes the labelpair and the length of the labelId list in the annotation, as well as the iurl.\n",
        "    imghtml = \"\"\"<b> Label: \"\"\"+ str(labelpair) +\"\"\"</b><br>\"\"\" + \"<img src=\"+iurl+\" width=200px; height=200px; style='float:left'>\"\n",
        "    display(HTML(imghtml))\n",
        "#The resulting HTML code displays an image with its associated labels for each of the bottom five annotations in the train_inp dataset based on the length of their labelId lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cebbbf4b-d217-4f67-b751-cb72ddfbf426",
        "_uuid": "1dbe4eee0cf261fa80482547eb6251c0aded430a",
        "id": "owf-wTmXa9Ri"
      },
      "source": [
        "## 1.6 Frequency Distribution of Images with respective Labels Counts in the dataset\n",
        "\n",
        "Lets visualize how many images are there in each label count bucket. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89d66cb5-cdca-49c6-a21c-532588eace13",
        "_uuid": "a74791dc0373c6ee23384ecbddb975ae9bb1bdb8",
        "collapsed": true,
        "id": "QeMeQVa8a9Ri"
      },
      "outputs": [],
      "source": [
        "#creating dictionary with keys equal to the unique lengths of labelId lists in srtedlist \n",
        "#and values equal to the frequency of those lengths.\n",
        "lbldst = Counter([len(x['labelId']) for x in srtedlist])\n",
        "\n",
        "labels = list(lbldst.keys()) #converting the keys of the dictionary into list\n",
        "values = list(lbldst.values()) #converting the values of the dictionary into list\n",
        "\n",
        "#plotting the unique length of labelId and values equal to the frequency of those lenghts\n",
        "trace1 = go.Bar(x=labels, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(10, 80, 190, 0.8)'))\n",
        "layout = dict(height=400, title='Frequency distribution of images with respective labels counts ', legend=dict(orientation=\"h\"));\n",
        "\n",
        "fig = go.Figure(data=[trace1], layout=layout);\n",
        "iplot(fig);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9a44f49f-f7cf-460e-9fcf-49919074110d",
        "_uuid": "8b1a31c52d386d31bfd26005cf0f014fde9689a3",
        "id": "r7TQ0eqxa9Rj"
      },
      "source": [
        "Most of the images in the dataset have 5 or 6 labels on an average. \n",
        "\n",
        "## 2. Colors Used in the Images \n",
        "\n",
        "In the e-commerce industry, colors play a very important role in the customer behaviours. Some people are more inclined towards soft colors while some prefer warm colors. In this section, lets visualize what type of colors are used in the images. \n",
        "\n",
        "## 2.1 Common Average Color of the Images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a71f987-f73d-40c8-898f-3937656006b5",
        "_kg_hide-input": true,
        "_uuid": "32e37de659f609957b76bc9b535f1133c7d37d4f",
        "collapsed": true,
        "id": "Tx4cO0eOa9Rk"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "from io import StringIO\n",
        "\n",
        "def compute_average_image_color(img):\n",
        "    width, height = img.size #getting width and height of image\n",
        "    count, r_total, g_total, b_total = 0, 0, 0, 0 #initializing the pixels count as zero\n",
        "    #iterating through every pixel in the image using two nested function\n",
        "    for x in range(0, width): \n",
        "        for y in range(0, height):\n",
        "            r, g, b = img.getpixel((x,y)) #getting the pixels of each images using getpixel method\n",
        "            #incrementing the red, green and blue with their respective pixel value obtained from the above getpixel method\n",
        "            r_total += r \n",
        "            g_total += g\n",
        "            b_total += b\n",
        "            count += 1\n",
        "    return (r_total/count, g_total/count, b_total/count) #returning the average of each colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f5e19ab-74a5-4a4e-afba-dcf824333941",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "fd1c286110ab4b6177dacba6a3bfbc8e82ea17c2",
        "collapsed": true,
        "id": "VV7XVYefa9Rl"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "imgpath = '../input/sampleimages/top_images/top_images/'\n",
        "read_from_disk = True\n",
        "\n",
        "#If read_from_disk is True, the code uses the os module to list all files in the imgpath directory using the os.listdir() function. \n",
        "#The resulting list of filenames is assigned to a variable called srtedlist.\n",
        "#If read_from_disk is False, the code assumes that there is a dictionary object called inp that contains an 'annotations' key, \n",
        "#and it sorts the list of annotations by the length of the 'labelId' field in descending order \n",
        "#using a lambda function and the sorted() function.The resulting sorted list is assigned to srtedlist.\n",
        "\n",
        "if read_from_disk:\n",
        "    srtedlist = os.listdir(imgpath)\n",
        "else:\n",
        "    srtedlist = sorted(inp['annotations'], key=lambda d: len(d['labelId']), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5690268a-238c-4292-b509-17503a54cbbb",
        "_kg_hide-input": true,
        "_uuid": "a237f9eb532504393411a8d6e480c993d3e8715d",
        "collapsed": true,
        "id": "McrO9m6qa9Rm"
      },
      "outputs": [],
      "source": [
        "average_colors = {}\n",
        "for img in srtedlist[:10]:\n",
        "    if read_from_disk:\n",
        "        img = Image.open(imgpath + img)\n",
        "    else:\n",
        "        iurli = get_image_url(img['imageId'])\n",
        "\n",
        "        ## download the images \n",
        "        # filename = iurli.split(\"/\")[-1].split(\"-large\")[0]\n",
        "        # urllib.urlretrieve(iurli, \"top_images/\"+filename)\n",
        "        \n",
        "        file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n",
        "        img = Image.open(img)\n",
        "           \n",
        "    average_color = compute_average_image_color(img) #computing the average of the downloaded image\n",
        "    \n",
        "    #If the resulting average color tuple is not already in the average_colors dictionary, the code adds it with a frequency of 0.\n",
        "    #The code then increments the frequency of the average color tuple by 1.\n",
        "    \n",
        "    if average_color not in average_colors:\n",
        "        average_colors[average_color] = 0\n",
        "    average_colors[average_color] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "30f53b7f-63c2-412c-987d-be3b5db06191",
        "_kg_hide-input": true,
        "_uuid": "14a19c795ac1bd2303d4d94dc56cc71587d7ddca",
        "collapsed": true,
        "id": "OBwYYcL0a9Ro"
      },
      "outputs": [],
      "source": [
        "# visualizing the frequency and distribution of average colors in a dataset of images, \n",
        "#specifically by displaying a color swatch for each unique average color tuple present in the average_colors dictionary.\n",
        "\n",
        "for average_color in average_colors:\n",
        "    average_color1 = (int(average_color[0]),int(average_color[1]),int(average_color[2]))\n",
        "    image_url = \"<span style='display:inline-block; min-width:200px; background-color:rgb\"+str(average_color1)+\";padding:10px 10px;'>\"+str(average_color1)+\"</span>\"\n",
        "#     print (image_url)\n",
        "    display(HTML(image_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2bb951c6-d66c-40f4-9bcd-03ba365bd2e1",
        "_uuid": "bb2ac3a1899c1ba90f672834d28b3247e2660a70",
        "id": "Xzv79-lia9Rp"
      },
      "source": [
        "## 2.2 Most Dominant Colors Used in the Images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "975a6d0c-0ffe-46ed-af36-37ca41e44b95",
        "_kg_hide-input": true,
        "_uuid": "c933fb6e9c02d906c37feb3472316755266f160a",
        "collapsed": true,
        "id": "spDP0YJHa9Rp"
      },
      "outputs": [],
      "source": [
        "## top used colors in images \n",
        "from colorthief import ColorThief\n",
        "import urllib \n",
        "\n",
        "pallets = []\n",
        "for img in srtedlist[:10]: #loops over the first 10 images in the srtedlist\n",
        "    \n",
        "    if read_from_disk:\n",
        "        img = imgpath + img\n",
        "    else:\n",
        "        iurli = get_image_url(img['imageId'])\n",
        "\n",
        "        ## download the images \n",
        "        # filename = iurli.split(\"/\")[-1].split(\"-large\")[0]\n",
        "        # urllib.urlretrieve(iurli, \"top_images/\"+filename)\n",
        "        \n",
        "        file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n",
        "        img = Image.open(img)\n",
        "        \n",
        "    #if the images are being read from disk or from a URL, and opens the image file using ColorThief method\n",
        "    color_thief = ColorThief(img)\n",
        "    \n",
        "    #extracting the dominant color of the image using the get_color() method, with a quality value of 1.\n",
        "    dominant_color = color_thief.get_color(quality=1)\n",
        "    \n",
        "    #An HTML code string is generated that contains a color swatch with a background color equal to the dominant color of the image.\n",
        "    image_url = \"<span style='display:inline-block; min-width:200px; background-color:rgb\"+str(dominant_color)+\";padding:10px 10px;'>\"+str(dominant_color)+\"</span>\"\n",
        "    #displaying the HTML code\n",
        "    display(HTML(image_url))\n",
        "    \n",
        "    #extracting a color palette of 6 colors from the image, and the resulting palette is appended to the pallets list.\n",
        "    palette = color_thief.get_palette(color_count=6)\n",
        "    pallets.append(palette)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fb146bdb-ca56-44ad-8145-21302ecb513c",
        "_uuid": "bb2f0c409ac9cbc9e40e3576e7c02c6c7a37b32b",
        "id": "h-b0o3ZIa9Rs"
      },
      "source": [
        "## 2.3 Common Color Pallets of the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8d21ac0-9be8-4e48-abec-82337428e039",
        "_kg_hide-input": true,
        "_uuid": "58519381c0c120993b4d639eb4e759b810c85e43",
        "collapsed": true,
        "id": "yGpxGXYba9Rt"
      },
      "outputs": [],
      "source": [
        "#taking the list of palettes generated in the previous code block and generates an HTML string for each palette. \n",
        "#For each palette, a string of HTML code is generated that displays a series of color swatches, with one swatch for each color in the palette\n",
        "\n",
        "for pallet in pallets:\n",
        "    img_url = \"\"\n",
        "    for pall in pallet:\n",
        "        img_url += \"<span style='background-color:rgb\"+str(pall)+\";padding:20px 10px;'>\"+str(pall)+\"</span>\"\n",
        "    img_url += \"<br>\"\n",
        "    display(HTML(img_url))\n",
        "    print \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7db10b16-9dae-4abd-bba1-71a775684848",
        "_uuid": "7b9aa71f07bddb15fddfd6bfcaaa52adbd9771a7",
        "id": "zCJPXUKya9Ru"
      },
      "source": [
        "## 3. Object Detection using TensorFlow API \n",
        "\n",
        "\n",
        "I have used tensorflow API for object detection the code is given in the following cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4223537b-7a6e-4369-bbf3-5a46acc5ca09",
        "_kg_hide-input": false,
        "_uuid": "2221c4e57ef7a494df5b1a61e8b58d8046b9d28f",
        "collapsed": true,
        "id": "t_AVm27oa9Rv"
      },
      "outputs": [],
      "source": [
        "### UNCOMMENT THE FOLLOWING LINE AFTER DOWNLOADING THE UTILS FROM THIS LINK - https://github.com/tensorflow/models/tree/master/research/object_detection/utils\n",
        "\n",
        "# from utils import label_map_util\n",
        "\n",
        "def DOWNLOAD_MODELS():\n",
        "    MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17' #defining the name of the path\n",
        "    MODEL_FILE = MODEL_NAME + '.tar.gz' #defining the file format\n",
        "    DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/' #defining the download base url\n",
        "    PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb' #defining the actual model file that is used for interface\n",
        "    PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt') #defining the path to the label map file\n",
        "    \n",
        "    #now downloading and extracting the pre-defined object detection model 'mobilenet-v1'\n",
        "    opener = urllib.request.URLopener()\n",
        "    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "    tar_file = tarfile.open(MODEL_FILE)\n",
        "    for file in tar_file.getmembers():\n",
        "        file_name = os.path.basename(file.name)\n",
        "        if 'frozen_inference_graph.pb' in file_name:\n",
        "            tar_file.extract(file, os.getcwd())\n",
        "\n",
        "def detect_object(filename):\n",
        "    \n",
        "    #creating an inner function tht converts image to numpy array\n",
        "    def img2array(img):\n",
        "        (img_width, img_height) = img.size\n",
        "        return np.array(img.getdata()).reshape((img_width, img_height, 3)).astype(np.uint8) #converting image to numpy array\n",
        "    \n",
        "    #Define 'categories' and 'probabilities' lists to store the detected object categories and their corresponding detection probabilities.\n",
        "    categories, probabilities = [], []\n",
        "    \n",
        "    #Loading the frozen inference graph of the model and create a TensorFlow session.\n",
        "    #Loading the label map and create a category index.\n",
        "    #Get the input and output tensors of the model graph.\n",
        "    \n",
        "    PATH_TO_CKPT = 'frozen_inference_graph.pb'\n",
        "    PATH_TO_LABELS = 'mscoco_label_map.pbtxt'\n",
        "    detection_graph = tf.Graph()\n",
        "    \n",
        "    with detection_graph.as_default():\n",
        "        od_graph_def = tf.GraphDef()\n",
        "        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "            serialized_graph = fid.read()\n",
        "            od_graph_def.ParseFromString(serialized_graph)\n",
        "            tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "    label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=100, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "    with detection_graph.as_default():\n",
        "        with tf.Session(graph=detection_graph) as sess:\n",
        "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "            \n",
        "            #Loading the input image, converting it to a numpy array, and expand its dimensions to match the input tensor shape of the model.\n",
        "            #Running the inference session with the input image and get the output tensors.\n",
        "            #Iterating over the detected objects and their corresponding scores, and append their categories and probabilities to the respective lists if the score is greater than 0.1.\n",
        "            \n",
        "            image = Image.open(filename)\n",
        "            image_np = img2array(image)\n",
        "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "            (boxes, scores, classes, num) = sess.run([detection_boxes, detection_scores, detection_classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n",
        "            for index,value in enumerate(classes[0]):\n",
        "                if float(scores[0,index]) > 0.1:\n",
        "                    temp =  category_index.get(value)['name']\n",
        "                    if temp not in categories:\n",
        "                        categories.append(temp)\n",
        "                        probabilities.append(scores[0,index])\n",
        "    return categories, probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f03c9c0b-873d-41f0-8355-452c71043efc",
        "_kg_hide-input": true,
        "_uuid": "47621e2304534f75d151e16182c2b16dad5c7beb",
        "collapsed": true,
        "id": "NkpIEScNa9Rw"
      },
      "outputs": [],
      "source": [
        "## UNCOMMENT THE FOLLOWING LINES TO RUN THE OBJECT DETECTION MODEL AND SAVE THE RESULTS \n",
        "\n",
        "# for img in srtedlist[:10]:\n",
        "#     iurli = get_image_url(img['imageId'])\n",
        "    \n",
        "#     file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n",
        "#     objects = detect_object(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "58b0a91b-8061-4d7e-abd2-28c91bffccfe",
        "_uuid": "7ae283e11fb78106db7724bf28d1b78124d5d7d3",
        "id": "6JfQ2zOHa9Rx"
      },
      "source": [
        "- Reference: [TensorFlow Object Detection Notebook](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)  \n",
        "- Pre-Trained Models Reference: [PreTrained Models](https://github.com/tensorflow/models/tree/676a4f70c20020ed41b533e0c331f115eeffe9a3/research/object_detection)  \n",
        "- Link to download the Utils: https://github.com/tensorflow/models/tree/master/research/object_detection/utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2917cbc0-941b-4732-aadd-84e8a8b2dba6",
        "_uuid": "5c6e3c854babec0caa3858d9bb5b2101f71f49c8",
        "id": "BrmJlh0Ra9Rx"
      },
      "source": [
        "Since it would have taken a lot of time on kaggle kernals, I have pre-computed the objects in my local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bcff6d5e-75b7-48e9-955c-c5308f7e226c",
        "_kg_hide-input": true,
        "_uuid": "3b91ab41a99253c5f65fc77511789eadfad32508",
        "collapsed": true,
        "id": "maBifI7ha9Ry"
      },
      "outputs": [],
      "source": [
        "objpath = '../input/precomputedobjects/objects.txt'\n",
        "\n",
        "objs = open(objpath).read().strip().split(\"\\n\")\n",
        "colors = [_ for _ in objs if \"color\" in _] #iterating over each element in object  and checks if the string color is present in the object\n",
        "non_colors = [_ for _ in objs if \"color\" not in _] #iterating over each element in object  and checks if the string color is not present in the object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "53fb49db-aa92-46e0-af08-70e70eb05592",
        "_uuid": "555c23073483ea9cbe4da0e9e677742fce55ba87",
        "id": "8sGE60pGa9Rz"
      },
      "source": [
        "## 3.1 Top Objects detected using Object detection "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a676f85c-1d81-43a0-9eba-a51ac94e44c3",
        "_kg_hide-input": true,
        "_uuid": "7668da79698d18ba6b1a20027c2a8a6c215c3452",
        "collapsed": true,
        "id": "-lj375eca9Rz"
      },
      "outputs": [],
      "source": [
        "#creating a word cloud visualization\n",
        "#Here  the 'WordCloud' class from the 'wordcloud' package is used to generate a word cloud visualization \n",
        "#based on the text in 'txt'. The word cloud is set to have a maximum font size of 50, a width of 600 pixels, \n",
        "#and a height of 300 pixels, and is generated using the 'generate' method of the 'WordCloud' class.\n",
        "txt = \"\"\n",
        "for i, color in enumerate(Counter(non_colors).most_common(100)):\n",
        "    txt += color[0]+\" \"\n",
        "wordcloud = WordCloud(max_font_size=50, width=600, height=300).generate(txt)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.title(\"Top Objects Detected in the images\", fontsize=15)\n",
        "plt.axis(\"off\")\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "22c818bb-376e-4249-a349-ec01b6c20222",
        "_uuid": "26afc2fc9b893404ad56a55ebb0361f5fdf2ed7d",
        "id": "vVx5WMbEa9R0"
      },
      "source": [
        "## 3.2 Top Color Detected in the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a93578c5-8d58-4c77-aeb7-87b4fa0ec4bf",
        "_kg_hide-input": true,
        "_uuid": "f2d1296b9a522aeaf947df93e3e0e40f75813b6d",
        "collapsed": true,
        "id": "leBWbhrVa9R1"
      },
      "outputs": [],
      "source": [
        "#creates a word cloud visualization for the most common color names in the 'colors' list.\n",
        "txt = \"\"\n",
        "for i, color in enumerate(Counter(colors).most_common(100)):\n",
        "    txt += (color[0] + \" \")\n",
        "txt = txt.replace(\"color\", \" \")\n",
        "wordcloud = WordCloud(max_font_size=50, width=600, height=300, background_color='white').generate(txt)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.title(\"Top Colors Used in the images\", fontsize=15)\n",
        "plt.axis(\"off\")\n",
        "plt.show() "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}